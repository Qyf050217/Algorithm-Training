import os
import csv
import re
import json
from datetime import datetime
from collections import defaultdict

# --- 1. è·¯å¾„è‡ªåŠ¨å®šä½ ---
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
ACCEPTED_DIR = os.path.join(BASE_DIR, 'Accepted')
ATTEMPTED_DIR = os.path.join(BASE_DIR, 'Attempted')
LOGS_DIR = os.path.join(BASE_DIR, 'DailyLogs')
HISTORY_CSV = os.path.join(BASE_DIR, 'training_history.csv')
README_FILE = os.path.join(BASE_DIR, 'README.md')

if not os.path.exists(LOGS_DIR): os.makedirs(LOGS_DIR)

# --- 2. é…ç½®å‚æ•° ---
TOP_N = 5

def get_oj_url(cpp_path):
    """é€‚é… CPH æ’ä»¶ï¼šä»åŒç›®å½•ä¸‹çš„ .cph æ–‡ä»¶å¤¹æå–ç½‘å€ [cite: 2026-01-30]"""
    cpp_filename = os.path.basename(cpp_path)
    cph_dir = os.path.join(os.path.dirname(cpp_path), '.cph')
    if os.path.exists(cph_dir) and os.path.isdir(cph_dir):
        for f in os.listdir(cph_dir):
            if f.endswith('.prob') and cpp_filename in f:
                try:
                    with open(os.path.join(cph_dir, f), 'r', encoding='utf-8') as f_in:
                        content = f_in.read()
                        url = re.search(r'https?://[^\s"\'\}]+', content)
                        if url: return url.group(0)
                except: continue
    return None

def scan_repository():
    """å…¨é‡æ‰«æä»“åº“ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ® [cite: 2026-01-30]"""
    daily_data = defaultdict(list)
    recent_ac_pool = []
    backlog_pool = []
    all_platforms = set()

    # 1. æ‰«æ Accepted (ç”¨äºæ—¥å¿—å’Œå†å²ç»Ÿè®¡)
    if os.path.exists(ACCEPTED_DIR):
        for root, _, files in os.walk(ACCEPTED_DIR):
            for f in files:
                if f.endswith('.cpp'):
                    path = os.path.join(root, f)
                    rel_parts = os.path.relpath(root, ACCEPTED_DIR).split(os.sep)
                    if len(rel_parts) >= 2:
                        platform, date_str = rel_parts[0], rel_parts[1]
                        all_platforms.add(platform)
                        url = get_oj_url(path)
                        mtime = os.path.getmtime(path)
                        
                        item = {
                            'name': f.replace('.cpp', ''), 'url': url, 'platform': platform,
                            'path': path, 'time': mtime, 'date': date_str
                        }
                        daily_data[date_str].append(item)
                        if url: recent_ac_pool.append(item)

    # 2. æ‰«æ Attempted (ç”¨äºç§¯å‹çœ‹æ¿)
    if os.path.exists(ATTEMPTED_DIR):
        for root, _, files in os.walk(ATTEMPTED_DIR):
            for f in files:
                if f.endswith('.cpp'):
                    path = os.path.join(root, f)
                    rel_root = os.path.relpath(root, ATTEMPTED_DIR)
                    platform = rel_root.split(os.sep)[0] if rel_root != "." else "Other"
                    url = get_oj_url(path)
                    if url:
                        backlog_pool.append({
                            'name': f.replace('.cpp', ''), 'url': url, 'platform': platform,
                            'time': os.path.getmtime(path)
                        })

    return daily_data, recent_ac_pool, backlog_pool, sorted(list(all_platforms))

def generate_daily_logs(daily_data):
    """ç”Ÿæˆ DailyLogs ä¸‹çš„ MD æ–‡ä»¶ [cite: 2026-01-30]"""
    for date_str, probs in daily_data.items():
        log_file = os.path.join(LOGS_DIR, f"{date_str}.md")
        content = [f"# ğŸ“ è®­ç»ƒæ€»ç»“: {date_str}\n\n", "| å¹³å° | é¢˜ç›®åç§° | æºç è·³è½¬ |\n| :--- | :--- | :--- |\n"]
        for p in probs:
            rel_code_path = os.path.relpath(p['path'], LOGS_DIR)
            name_display = f"[{p['name']}]({p['url']})" if p['url'] else f"**{p['name']} (æœ¬åœ°)**"
            content.append(f"| `{p['platform']}` | {name_display} | [æŸ¥çœ‹ä»£ç ]({rel_code_path}) |\n")
        with open(log_file, 'w', encoding='utf-8') as f:
            f.writelines(content)

def update_readme_and_csv(daily_data, recent_pool, backlog_pool, platforms):
    """æ›´æ–° README çœ‹æ¿ã€å¸¦é“¾æ¥çš„å†å²è¡¨ï¼Œå¹¶æŒä¹…åŒ– CSV [cite: 2026-01-30]"""
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # çœ‹æ¿æ’åº
    recent_ac = sorted(recent_pool, key=lambda x: x['time'], reverse=True)[:TOP_N]
    oldest_pending = sorted(backlog_pool, key=lambda x: x['time'], reverse=False)[:TOP_N]

    # 1. æ„é€  Dashboard HTML
    dashboard_html = f"""
<table width="100%">
    <tr>
        <td width="50%" valign="top">
            <h4 align="center">âœ… æœ€è¿‘ AC ({len(recent_ac)})</h4>
            <table width="100%">
                <thead><tr><th align="center">å¹³å°</th><th align="left">é¢˜ç›®</th><th align="right">æ—¥æœŸ</th></tr></thead>
                <tbody>
                {"".join([f"<tr><td align='center'><code>{p['platform']}</code></td><td><a href='{p['url']}'>{p['name']}</a></td><td align='right'>{datetime.fromtimestamp(p['time']).strftime('%m-%d')}</td></tr>" for p in recent_ac]) if recent_ac else "<tr><td colspan='3' align='center'>æš‚æ— è®°å½•</td></tr>"}
                </tbody>
            </table>
        </td>
        <td width="50%" valign="top">
            <h4 align="center">âŒ› ç§¯å‹æœ€ä¹… ({len(oldest_pending)})</h4>
            <table width="100%">
                <thead><tr><th align="center">å¹³å°</th><th align="left">é¢˜ç›®</th><th align="right">æ—¥æœŸ</th></tr></thead>
                <tbody>
                {"".join([f"<tr><td align='center'><code>{p['platform']}</code></td><td><a href='{p['url']}'>{p['name']}</a></td><td align='right'>{datetime.fromtimestamp(p['time']).strftime('%m-%d')}</td></tr>" for p in oldest_pending]) if oldest_pending else "<tr><td colspan='3' align='center'>è¡¥é¢˜æ•ˆç‡æ»¡åˆ†ï¼</td></tr>"}
                </tbody>
            </table>
        </td>
    </tr>
</table>
"""

    # 2. æ„é€  README å†…å®¹
    content = [
        "# ğŸ† Algorithm Training Log\n",
        f"> ğŸ¯ **Goal:** ACM Silver Medal | *Last updated: {now}*\n\n",
        "## ğŸ“ˆ Heatmap\n",
        "![Algorithm Training Heatmap](ac_heatmap.png)\n\n",
        "--- \n\n",
        "## ğŸ•’ Dashboard\n",
        dashboard_html + "\n",
        "--- \n\n",
        "## ğŸ“Š AC History\n\n"
    ]

    # 3. æ„é€  AC å†å²è¡¨ (æ—¥æœŸå¸¦é“¾æ¥) å¹¶å‡†å¤‡ CSV è®°å½•
    if daily_data:
        content.append("| æ—¥æœŸ | " + " | ".join(platforms) + " | **Total** |\n")
        content.append("| :--- | " + " | ".join([":---:"] * len(platforms)) + " | :---: |\n")
        
        csv_rows = []
        for d in sorted(daily_data.keys(), reverse=True):
            counts = {p: len([x for x in daily_data[d] if x['platform'] == p]) for p in platforms}
            total = sum(counts.values())
            
            # README è¡Œï¼šæ—¥æœŸæŒ‡å‘ DailyLogs [cite: 2026-01-30]
            row_md = [str(counts[p]) if counts[p] > 0 else "-" for p in platforms]
            content.append(f"| [{d}](./DailyLogs/{d}.md) | " + " | ".join(row_md) + f" | **{total}** |\n")
            
            # CSV è¡Œè®°å½• (æŒ‰æ­£åºä¿å­˜)
            csv_rows.append({'Date': d, **counts, 'Total': total})

        # 4. å†™å…¥ README
        with open(README_FILE, 'w', encoding='utf-8') as f:
            f.writelines(content)

        # 5. æŒä¹…åŒ– CSV (æŒ‰æ—¥æœŸæ­£åºå†™å…¥)
        headers = ['Date'] + platforms + ['Total']
        with open(HISTORY_CSV, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            writer.writerows(sorted(csv_rows, key=lambda x: x['Date']))

if __name__ == "__main__":
    d_data, ac_pool, todo_pool, p_list = scan_repository()
    generate_daily_logs(d_data)
    update_readme_and_csv(d_data, ac_pool, todo_pool, p_list)
    print(f"âœ… æ•´åˆåŒæ­¥å®Œæˆï¼šREADME å¯¼èˆªã€æ¯æ—¥æ€»ç»“åŠ CSV æ•°æ®å·²å…¨éƒ¨æ›´æ–°ã€‚")